{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a690fb",
   "metadata": {},
   "source": [
    "Zunächst wird eine Verbindung zu einer SQL Server-Datenbank hergestellt und es werden Aktiendaten für ein spezifisches Unternehmen extrahiert. Zur weiteren Bearbeitung wird eine Transformation der Daten angewandt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76464cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "# Verbindungsstring\n",
    "conn_str = (\n",
    "    r'Driver=SQL Server;'\n",
    "    r'Server=.\\SQLEXPRESS;'\n",
    "    r'Database=studienprojekt;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "# Verbindung zur Datenbank herstellen\n",
    "cnxn = pyodbc.connect(conn_str)\n",
    "\n",
    "# SQLAlchemy connectable erstellen\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: cnxn)\n",
    "\n",
    "# Tabelle prüfen\n",
    "# BITTE HIER DATUM UND COMPANY ÄNDERN\n",
    "table_name = \"stock_data\"  # Name der Zieltabelle\n",
    "start_date = \"2020-07-01\"  # Startdatum, ab dem Daten überprüft werden sollen\n",
    "end_date = \"2023-07-01\"  # Enddatum, bis zu dem Daten überprüft werden sollen\n",
    "company_name = \"AAPL\"  # nur AAPL, MSFT oder GOOG möglich // Name des Unternehmens, für das Daten geladen werden sollen\n",
    "\n",
    "# Daten aus der Tabelle für das bestimmte Unternehmen in einen DataFrame laden\n",
    "select_query = f\"SELECT * FROM {table_name} WHERE date >= ? AND date <= ? AND Company = ?\"\n",
    "data = pd.read_sql(select_query, con=engine, params=(start_date, end_date, company_name))\n",
    "\n",
    "# added\n",
    "data[\"ID\"] = data.index\n",
    "data = data[[\"ID\", \"Date\", \"Company\", \"Type\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verbindung schließen\n",
    "cnxn.close()\n",
    "\n",
    "# DataFrame anzeigen\n",
    "print(data.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06983d4d",
   "metadata": {},
   "source": [
    "Der historische Kursverlauf der Close-Werte wird in einemLiniendiagramm visualisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207449fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(data[\"Date\"], data[\"Close\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8272479",
   "metadata": {},
   "source": [
    "`seasonal_decompose` ist eine Funktion, die eine Zeitreihe in drei verschiedene Komponenten zerlegt:\n",
    "\n",
    "- Trend: Die zugrundeliegende Tendenz der Zeitreihen. Dabei werden steigende und fallende Tendenzen abgebildet.\n",
    "- Saisonalität: Periodische Schwankungen. Zum Beispiel könnte der Aktienkurs tendenziell während bestimmter Zeiten eines Jahres steigen oder fallen.\n",
    "- Residual: Der Teil der Zeitreihe, der nicht durch Trend und Saisonalität erklärt werden kann.\n",
    "\n",
    "Folgende Parameter werden verwendet:\n",
    "\n",
    "- `data[\"Close\"]`: Die zu analysierende Zeitreihe der Close-Werte\n",
    "- `model='multiplicative'`: Das zu verwendende Modell für die Zerlegung. Ein multiplikatives Modell ist geeignet, wenn die Amplitude der saisonalen Schwankungen mit der Zeit zunimmt oder abnimmt.\n",
    "- `period=30`: Die Länge des saisonalen Zyklus. In diesem Fall wird davon ausgegangen, dass sich die saisonalen Schwankungen alle 30 Tage wiederholen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46daf5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(data[\"Close\"], model='multiplicative', period=30)\n",
    "fig = plt.figure()  \n",
    "fig = result.plot() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f870720",
   "metadata": {},
   "source": [
    "Die Funktion `pd.plotting.autocorrelation_plot()` ist eine visuelle Hilfsmethode, um die Autokorrelation in einer Zeitreihe darzustellen. Autokorrelation, auch bekannt als serielle Korrelation, bezeichnet die Korrelation eines Elements in einer Serie mit anderen Elementen aus derselben Serie, die von früheren Zeitschritten stammen.\n",
    "\n",
    "Das Autokorrelationsdiagramm hilft zu erkennen, ob es eine Muster oder eine Beziehung zwischen den Werten einer Zeitspanne gibt und den Werten aus vorherigen Zeitspannen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd12540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(data[\"Close\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fae556",
   "metadata": {},
   "source": [
    "Die Funktion `plot_pacf` aus der `statsmodels.graphics.tsaplots` Bibliothek wird verwendet, um das partielle Autokorrelationsdiagramm für die gegebenen Zeitreihendaten zu erstellen. \n",
    "\n",
    "Ein Diagramm der partiellen Autokorrelation gibt Auskunft über die direkte Beziehung eines gegebenen Lags mit der aktuellen Zeitspanne, nachdem die Effekte aller kleineren Lags herausgerechnet wurden. \n",
    "\n",
    "Hier sind die Parameter, die in der Funktion verwendet werden:\n",
    "\n",
    "- `data[\"Close\"]`: Die zu analysierende Zeitreihe (in diesem Fall die Schlusskurse der Aktien).\n",
    "- `lags=100`: Die Anzahl der Lags, die im Diagramm angezeigt werden sollen. Ein Lag ist eine zeitliche Verzögerung.\n",
    "- `method='ywm'`: Die Methode zur Schätzung der partiellen Autokorrelation. 'ywm' steht für die Yule-Walker-Gleichungen mit Modifikationen entsprechend der Methode von Levinson-Durbin.\n",
    "\n",
    "Insgesamt zeigt dieser Codeabschnitt ein Diagramm, das die partielle Autokorrelation der Schlusskurse über 100 Lags hinweg darstellt. Diese Art von Diagramm kann nützlich sein, um die Anzahl der Lags zu bestimmen, die in einem autoregressiven Prognosemodell verwendet werden sollten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e9f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(data[\"Close\"], lags=100, method='ywm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329d7d1",
   "metadata": {},
   "source": [
    "ARIMA ist ein gebräuchlicher Algorithmus für die Vorhersage von Zeitreihen. Er basiert auf drei zentralen Parametern:\n",
    "\n",
    "- `p` steht für die Anzahl der zurückliegenden Datenpunkte, die berücksichtigt werden, um den nächsten Punkt zu prognostizieren. Dies ist der autoregressive Aspekt des Modells.\n",
    "- `d` repräsentiert die notwendigen Differenzbildungen, um die Zeitreihe stationär zu machen. Dies ist der integrierte Aspekt des Modells.\n",
    "- `q` bezieht sich auf die Anzahl der vorhergehenden Fehler, die in das Modell einfließen. Dies ist der gleitende Durchschnittsaspekt des Modells.\n",
    "\n",
    "In der Funktion `auto_arima` wird das 'Close'-Feld aus unseren Börsendaten als eindimensionales Array übergeben. Die Funktion probiert verschiedene Kombinationen von `p`, `d` und `q` aus und wählt diejenige aus, die das beste Modell ergibt (das heißt, das Modell mit dem niedrigsten AIC-Wert).\n",
    "\n",
    "Die Einstellung `seasonal=False` teilt dem Modell mit, dass es keine saisonalen Komponenten berücksichtigen soll, während `trace=True` bewirkt, dass der Fortschritt der Funktion während der Ausführung angezeigt wird.\n",
    "\n",
    "Zum Schluss werden die optimalen Werte für `p`, `d` und `q` ausgegeben und in den entsprechenden Variablen gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae14c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Annahme: Sie haben Ihre Zeitreihendaten in einem DataFrame namens 'data' geladen\n",
    "\n",
    "# Die Zeitreihendaten in eine eindimensionale NumPy-Array-ähnliche Struktur umwandeln\n",
    "y = np.array(data['Close'])\n",
    "\n",
    "# AutoARIMA-Modell erstellen und anpassen, um die optimalen Parameterwerte zu ermitteln\n",
    "model = auto_arima(y, seasonal=False, trace=True)\n",
    "\n",
    "# Die optimalen Parameterwerte für P, D und Q ausgeben\n",
    "print(f\"Optimale Werte für P, D und Q: {model.order}\")\n",
    "p, d, q = model.order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1c12d",
   "metadata": {},
   "source": [
    "Es wurden verschiedene Kombinationen von P, D und Q getestet und dabei jeweils den AIC (Akaike Information Criterion) berechnet. Die AIC ist eine Maßzahl für die Güte eines statistischen Modells: Je kleiner der AIC-Wert, desto besser passt das Modell zu den Daten, wenn man die Komplexität des Modells mit einbezieht.\n",
    "\n",
    "Im gegebenen Fall war das beste Modell ARIMA(0,1,0), weil es den niedrigsten AIC-Wert hatte (3679.890). Die optimalen Werte für P, D und Q waren also 0, 1 und 0. Dies bedeutet:\n",
    "\n",
    "    P (Ordnung des autoregressiven Teils): 0 – Es werden keine vorherigen Datenpunkte zur Vorhersage des nächsten Punktes verwendet.\n",
    "    D (Integrationsgrad): 1 – Die Zeitreihe wurde einmal differenziert, um sie stationär zu machen.\n",
    "    Q (Ordnung des gleitenden Durchschnitts): 0 – Es werden keine vorherigen Fehler in das Modell einbezogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1470065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = fitted.predict()\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c800d",
   "metadata": {},
   "source": [
    "SARIMAX wird verwendet, um ein saisonales autoregressives, integriertes gleitendes Durchschnittsmodell (Seasonal AutoRegressive Integrated Moving Average, SARIMA) zu erstellen und an die Daten anzupassen. Dies ist eine Erweiterung des ARIMA-Modells, das die saisonale Komponente in den Daten berücksichtigt.\n",
    "\n",
    "Es werden 4 Parameter übernommen: die Zeitreihendaten, die Ordnung des Modells (p, d und q), und die saisonale Ordnung des Modells. Hier sind die Schlusskurse ('Close') aus den Aktiendaten die Zeitreihendaten und die durch auto_arima ermittelten optimalen Werte werden als Ordnung des Modells und der saisonalen Ordnung übergeben. Die Zahl 12 in der saisonalen Ordnung bedeutet, dass die Saisonalität auf jährlicher Basis betrachtet wird (12 Monate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39015a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "model = sm.tsa.statespace.SARIMAX(data['Close'],\n",
    "                                 order=(p, d, q),\n",
    "                                 seasonal_order=(p, d, q, 12))\n",
    "model = model.fit()\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "# Gesamtdauer berechnen\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(model.summary())\n",
    "print(\"Durchlaufdauer:\", duration, \"Sekunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04f498",
   "metadata": {},
   "source": [
    "Es wird eine Vorhersage für die nächsten 30 Tage gemacht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15220949",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(len(data), len(data)+30)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c497334",
   "metadata": {},
   "source": [
    "Die Vorhersage der nächsten 30 Tage wird nun visualisiert. Es werden die historischen Daten mit denen trainiert wurde und die Vorhersage in einem Liniendiagramm abgebildet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ad99d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[\"Close\"].plot(legend=True, label=\"Training Data\", figsize=(15, 10))\n",
    "predictions.plot(legend=True, label=\"Predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7363210",
   "metadata": {},
   "source": [
    "Zuerst werden zukünftige Daten generiert, indem ein `pd.date_range`-Objekt erstellt wird, das mit dem Tag nach dem letzten Datum in den Daten beginnt und für die nächsten 30 Tage andauert.\n",
    "\n",
    "Ein neuer DataFrame `df_predictions` wird erstellt, der die zukünftigen Daten und die entsprechenden Vorhersagen enthält.\n",
    "\n",
    "Dann wird dieser neue DataFrame an den ursprünglichen DataFrame angehängt, um einen erweiterten DataFrame `data_extended` zu erstellen, der sowohl die ursprünglichen Daten als auch die Vorhersagen enthält.\n",
    "\n",
    "Anschließend wird ein Plot erstellt, der die Schlusskurse mit Datum abbildet. Die ursprüngliche Zeitreihe wird zusammen mit den Vorhersagen für die nächsten 30 Tage gezeichnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d89a9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vorhersagen für die nächsten 30 Tage erhalten\n",
    "#predictions = model.predict(len(data), len(data) + 29)  # 30 Tage Vorhersagen\n",
    "\n",
    "# Daten für die nächsten 30 Tage generieren\n",
    "future_dates = pd.date_range(start=data['Date'].iloc[-1] + timedelta(days=1), periods=30)\n",
    "\n",
    "# Ein DataFrame für die Vorhersagen erstellen\n",
    "df_predictions = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Close': predictions\n",
    "})\n",
    "\n",
    "# DataFrame erweitern, indem Sie die Vorhersagen anhängen\n",
    "data_extended = pd.concat([data, df_predictions])\n",
    "\n",
    "# Plot erstellen\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data_extended['Date'], data_extended['Close'], label='Original')\n",
    "plt.plot(data_extended['Date'].tail(30), data_extended['Close'].tail(30), label='Vorhersagen')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Close')\n",
    "plt.title('Vorhersagen für Close-Preise')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf38816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Key einbinden\n",
    "company = data.loc[0]['Company']\n",
    "data_extended['Company'] = company\n",
    "data_extended['Type'] = data_extended['High'].apply(lambda x: f'Arima_{company}' if np.isnan(x) else f'Act_{company}')\n",
    "data_extended.loc[data_extended['Type'].str.contains('Arima'), 'Forecast_Date'] = end_date\n",
    "data_extended['Key'] = data_extended['Company'] + '_' + data_extended['Date'].astype(str) + '_' + data_extended['Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dd65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Deaktivieren der Pandas-Warnungen\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Verbindungsstring\n",
    "conn_str = (\n",
    "    r'Driver=SQL Server;'\n",
    "    r'Server=.\\SQLEXPRESS;'\n",
    "    r'Database=studienprojekt;'\n",
    "    r'Trusted_Connection=yes;'\n",
    ")\n",
    "\n",
    "# Verbindung zur Datenbank herstellen\n",
    "cnxn = pyodbc.connect(conn_str)\n",
    "\n",
    "# SQLAlchemy connectable erstellen\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: cnxn)\n",
    "\n",
    "# Query erstellen, um alle Daten auszulesen\n",
    "query = \"SELECT * FROM stock_data_forecast\"\n",
    "\n",
    "# Ausführen der Query und Laden der Ergebnisse in einen DataFrame\n",
    "exists_df = pd.read_sql(query, cnxn)\n",
    "\n",
    "# added\n",
    "exists_df = exists_df.astype(data_extended.dtypes)\n",
    "exists_df = exists_df.reindex(columns=data_extended.columns)\n",
    "\n",
    "# Key einbinden\n",
    "exists_df['Key'] = exists_df['Company'] + '_' + exists_df['Date'].astype(str) + '_' + exists_df['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50591d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenführen der beiden DataFrames\n",
    "combined_df = pd.concat([exists_df, data_extended])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernen von Duplikaten\n",
    "combined_df = combined_df.drop_duplicates(subset=['Key'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b66e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schreiben des aktualisierten DataFrames in die SQL Server-Tabelle\n",
    "combined_df.to_sql('stock_data_forecast', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Anzahl der geschriebenen Zeilen erhalten\n",
    "num_written_rows = combined_df.shape[0]\n",
    "print(f\"Es wurden {num_written_rows} Zeilen erfolgreich übertragen.\")\n",
    "\n",
    "exists_df = pd.read_sql('SELECT * FROM stock_data_forecast', cnxn)\n",
    "exists_df.drop_duplicates(subset='Key', inplace=True)\n",
    "\n",
    "# Verbindung schließen\n",
    "cnxn.close()\n",
    "print(f\"Die Verbindung ist wieder geschlossen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfacab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darstellung der übertragenen Daten\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bd902",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
